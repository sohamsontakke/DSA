{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11275b4f-987d-43a2-a78e-87da3304f7fb",
   "metadata": {},
   "source": [
    "#IR 1\n",
    "# IR Practical 1 : Write a program for pre-processing of a text document such as stop word removal, stemming.\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "text_document = \"Text preprocessing is an essential step in natural language processing.\"\n",
    "\n",
    "words = word_tokenize(text_document)\n",
    "print(words)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "\n",
    "print(\"Filtered Words:\", filtered_words)\n",
    "print(\"Stemmed Words:\", stemmed_words)\n",
    "print(\"Lemmatized Words:\", lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90119ee4-2527-4c03-94a4-7fa039ba3686",
   "metadata": {},
   "source": [
    "#IR2\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# Sample documents\n",
    "documents = {\n",
    "    1: \"The quick brown fox jumps over the lazy dog.\",\n",
    "    2: \"Never jump over the lazy dog quickly.\",\n",
    "    3: \"A fast-moving fox is better than a slow dog.\"\n",
    "}\n",
    "\n",
    "# Build the inverted index\n",
    "index = defaultdict(set)\n",
    "for doc_id, text in documents.items():\n",
    "    for word in re.findall(r'\\w+', text.lower()):\n",
    "        index[word].add(doc_id)\n",
    "\n",
    "# Search for query in the inverted index\n",
    "query = input(\"Enter search query: \")\n",
    "words = query.lower().split()\n",
    "result = set.intersection(*(index.get(word, set()) for word in words))\n",
    "\n",
    "# Print matching documents\n",
    "print(\"Matching documents:\", sorted(result) if result else \"No match found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436fe465-4639-402d-89c9-792e776fc067",
   "metadata": {},
   "source": [
    "#IR3\n",
    "import pandas as pd\n",
    "from pgmpy.models import BayesianNetwork\n",
    "from pgmpy.estimators import BayesianEstimator\n",
    "from pgmpy.inference import VariableElimination\n",
    "\n",
    "data = pd.read_csv(\"heart_cleveland_upload.csv\")\n",
    "\n",
    "model = BayesianNetwork([\n",
    "    ('age', 'condition'),\n",
    "    ('sex', 'condition'),\n",
    "    ('cp', 'condition'),\n",
    "    ('trestbps', 'condition'),\n",
    "    ('chol', 'condition')\n",
    "])\n",
    "\n",
    "model.fit(data, estimator=BayesianEstimator)\n",
    "HeartDisease_infer = VariableElimination(model)\n",
    "\n",
    "q1 = HeartDisease_infer.query(variables=['condition'], evidence={'cp': 0})\n",
    "print(\"CP\",q1)\n",
    "\n",
    "q2 = HeartDisease_infer.query(variables=['condition'], evidence={'trestbps': 160})\n",
    "print(\"TRESTBPS\",q2)\n",
    "\n",
    "q3 = HeartDisease_infer.query(variables=['condition'], evidence={'age': 69})\n",
    "print(\"AGE\",q3)\n",
    "\n",
    "q4 = HeartDisease_infer.query(variables=['condition'], evidence={'chol': 243})\n",
    "print(\"CHOL\",q4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c25029-4478-4534-b456-01c0fdfffe29",
   "metadata": {},
   "source": [
    "#IR4\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from warnings import filterwarnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "df = pd.read_csv(\"email_spam.csv\")\n",
    "df.head()\n",
    "\n",
    "df1 = df\n",
    "df1.duplicated().sum()\n",
    "\n",
    "df2 = df1.drop_duplicates()\n",
    "df2.duplicated().sum()\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df2['spam'] = df2[\"type\"]\n",
    "df2['spam'] = le.fit_transform(df2['spam'])\n",
    "df2.head()\n",
    "\n",
    "df2[\"text\"] = df2[\"text\"].replace(\"\\n\",\"\", regex=True)\n",
    "df2.head()\n",
    "x_train, x_test, y_train, y_test = train_test_split(df2['text'], df2['spam'], test_size = 0.1)\n",
    "\n",
    "v = CountVectorizer()\n",
    "x_train_count = v.fit_transform(x_train.values)\n",
    "x_test_count = v.transform(x_test)\n",
    "x_train_count.toarray()\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train_count, y_train)\n",
    "\n",
    "model.score(x_test_count, y_test)\n",
    "\n",
    "email = ['take any one from dataset ']\n",
    "\n",
    "new_email = v.transform(email)\n",
    "ans = model.predict(new_email)\n",
    "if ans[0] == 1:\n",
    "    print(\"Spam\")\n",
    "else:\n",
    "    print(\"Not Spam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711a7562-5a82-4061-969e-7beaa7f85d51",
   "metadata": {},
   "source": [
    "#IR5\n",
    "# IR Practical : 5 Implement Agglomerative hierarchical clustering algorithm using appropriate dataset.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "n_clusters = 3\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n",
    "cluster_labels = agg_clustering.fit_predict(X_pca)\n",
    "\n",
    "linked = linkage(X_pca, 'ward')\n",
    "plt.figure(figsize=(12, 6))\n",
    "dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=2)\n",
    "plt.title('Dendrogram')\n",
    "plt.xlabel('Cluster Size')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n",
    "\n",
    "print(\"Cluster Labels:\")\n",
    "print(cluster_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfc477-e794-4c22-880b-7476ffb0c55b",
   "metadata": {},
   "source": [
    "#BI!\n",
    "#Read the DNA sequence from file\n",
    "with open('dna_sequence.txt', 'r') as file:\n",
    "    dna_sequence = file.read().strip()\n",
    "\n",
    "dna_sequence\n",
    "\n",
    "# 1. Function to calculate GC content\n",
    "def calculate_gc_content(sequence):\n",
    "    g_count = sequence.count('G')\n",
    "    c_count = sequence.count('C')\n",
    "    gc_content = (g_count + c_count) / len(sequence) * 100\n",
    "    return round(gc_content,2)\n",
    "\n",
    "# 2. Function to find motifs in the DNA sequence\n",
    "def find_motifs(sequence, motif):\n",
    "    positions = []\n",
    "    for i in range(len(sequence) - len(motif) + 1):\n",
    "        if sequence[i:i + len(motif)] == motif:\n",
    "            positions.append(i)\n",
    "    return positions\n",
    "\n",
    "# 3. Function to identify coding regions (start and stop codons)\n",
    "def find_coding_regions(sequence):\n",
    "    start_codon = \"ATG\"\n",
    "    stop_codons = [\"TAA\", \"TAG\", \"TGA\"]\n",
    "    coding_regions = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(sequence) - 2:\n",
    "        codon = sequence[i:i + 3]\n",
    "        if codon == start_codon:\n",
    "            # Find the next stop codon after the start codon\n",
    "            for j in range(i + 3, len(sequence) - 2, 3):\n",
    "                stop_codon = sequence[j:j + 3]\n",
    "                if stop_codon in stop_codons:\n",
    "                    coding_regions.append((i, j + 3))  # coding region from start to the end of stop codon\n",
    "                    i = j + 3\n",
    "                    break\n",
    "        i += 3\n",
    "\n",
    "    return coding_regions\n",
    "\n",
    "# Calculate results\n",
    "gc_content = calculate_gc_content(dna_sequence)\n",
    "motif_positions = find_motifs(dna_sequence, \"ATG\")\n",
    "coding_regions = find_coding_regions(dna_sequence)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"GC content : {gc_content}%\")\n",
    "\n",
    "print(f\"Motif ATG found at positions : {motif_positions}\")\n",
    "\n",
    "if coding_regions:\n",
    "    print(\"Coding regions : \")\n",
    "    for start, end in coding_regions:\n",
    "        print(f\"Start: {start}, End: {end}\")\n",
    "        print(f\"Sequence : {dna_sequence[start:end]}\")\n",
    "else:\n",
    "    print(\"No coding regions found in the sequence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03523b6b-aeb1-41f2-b45e-ce464ee9c5c6",
   "metadata": {},
   "source": [
    "#BI2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 1: Simulate RNA-Seq Dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "genes = [f'gene_{i}' for i in range(1, 101)]\n",
    "conditions = ['Control', 'Treatment']\n",
    "\n",
    "samples = [f'sample_{i}' for i in range(1, 11)]\n",
    "data = np.random.poisson(lam=20, size=(100, 10))\n",
    "\n",
    "# Simulate differential expression for some genes in Treatment condition\n",
    "data[0:5, 5:10] += 15\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data, index=genes, columns=samples)\n",
    "\n",
    "metadata = pd.DataFrame({'sample': samples,\n",
    "                          'condition': ['Control']*5 + ['Treatment']*5})\n",
    "\n",
    "# Step 2: Normalize the Data\n",
    "df_norm = df.div(df.sum(axis=0), axis=1) * 10**6\n",
    "df_log = np.log2(df_norm + 1)\n",
    "\n",
    "def differential_expression(df, metadata):\n",
    "    results = []\n",
    "    for gene in df.index:\n",
    "        y = df_log.loc[gene].values\n",
    "        X = pd.get_dummies(metadata['condition'], drop_first=True)\n",
    "        # The line below was modified to cast the DataFrame to float\n",
    "        X = sm.add_constant(X.astype(float))\n",
    "        model = sm.OLS(y, X).fit()\n",
    "        p_value = model.pvalues[1]\n",
    "        results.append({'gene': gene, 'p_value': p_value})\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df['adjusted_p_value'] = sm.stats.multipletests(results_df['p_value'], method='fdr_bh')[1]\n",
    "\n",
    "    return results_df\n",
    "\n",
    "# Call the differential_expression function to calculate results_df\n",
    "results_df = differential_expression(df_log, metadata)  # This line was added to call the function\n",
    "\n",
    "# Filter differentially expressed genes\n",
    "deg = results_df[results_df['adjusted_p_value'] < 0.05]\n",
    "\n",
    "# Step 4: Functional Annotation (Simulated Annotations)\n",
    "annotations = {\n",
    "    'gene_1': 'Pathway A',\n",
    "    'gene_2': 'Pathway B',\n",
    "    'gene_3': 'Pathway C',\n",
    "    'gene_4': 'Pathway D',\n",
    "    'gene_5': 'Pathway E',\n",
    "}\n",
    "deg['annotation'] = deg['gene'].map(annotations).fillna('Unknown')\n",
    "\n",
    "# Step 5: Biological Interpretation (Plotting)\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.scatterplot(x='gene', y='adjusted_p_value', hue='annotation', data=deg)\n",
    "plt.axhline(y=0.05, color='r', linestyle='--')\n",
    "\n",
    "plt.xlabel('Genes')\n",
    "plt.ylabel('Adjusted P-Value')\n",
    "\n",
    "plt.title('Differentially Expressed Genes')\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.legend(title='Annotations')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Save results to a CSV file\n",
    "deg.to_csv('differentially_expressed_genes.csv', index=False)\n",
    "\n",
    "# Generate the Report\n",
    "report = f\"\"\"\n",
    "RNA-Seq Data Analysis Report\n",
    "\n",
    "Differentially Expressed Genes\n",
    "\n",
    "{deg[['gene', 'adjusted_p_value']]}\n",
    "\n",
    "Functional Annotations\n",
    "\n",
    "{deg[['gene', 'annotation']]}\n",
    "\n",
    "Potential Biological Interpretations\n",
    "\n",
    "The genes gene_1, gene_2, etc., are involved in pathways A, B, etc.\n",
    "These pathways are important for understanding the effect of the treatment condition.\n",
    "\"\"\"\n",
    "\n",
    "# Save the report to a text file\n",
    "with open('RNASeq_Analysis_Report.txt', 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(\"Analysis complete. Results saved to 'differentially_expressed_genes.csv' and 'RNASeq_Analysis_Report.txt'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719d7b3d-1e0e-47b0-b914-fbfa81cbe9f6",
   "metadata": {},
   "source": [
    "#BI3\n",
    "import torch\n",
    "import sidechainnet as scn\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "from visualize import build_visualizable_structures, plot_protein\n",
    "from model import ProteinNet\n",
    "from trainer import train\n",
    "from config import get_parameters\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# To train with a GPU, go to Runtime > Change runtime type\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using {device} for training.\")\n",
    "def main(config, dataloader):\n",
    "    print(\"Available Dataloaders =\", list(dataloader.keys()))\n",
    "\n",
    "    # Create the model and move it to the GPU\n",
    "    model = ProteinNet(d_hidden=config.d_hidden,\n",
    "                            dim=config.dim,\n",
    "                            d_in=config.d_in,\n",
    "                            d_embedding=config.d_embedding,\n",
    "                            heads = config.n_heads,\n",
    "                            dim_head = config.head_dim,\n",
    "                            integer_sequence=config.integer_sequence)\n",
    "    model = model.to(device)\n",
    "\n",
    "    trained_model = train(model, config, dataloader, device)\n",
    "    if os.path.exists(config.model_save_path)==False:\n",
    "        os.mkdir(config.model_save_path)\n",
    "    torch.save(trained_model.state_dict(), '{}/model_weights.pth'.format(config.model_save_path))\n",
    "\n",
    "def plot(idx, dataloader, config):\n",
    "    model =  ProteinNet(d_hidden=config.d_hidden,\n",
    "                        dim=config.dim,\n",
    "                        d_in=config.d_in,\n",
    "                        d_embedding=config.d_embedding,\n",
    "                        heads = config.n_heads,\n",
    "                        dim_head = config.head_dim,\n",
    "                        integer_sequence=config.integer_sequence)\n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(torch.load('{}/model_weights.pth'.format(config.model_save_path)))\n",
    "\n",
    "    if os.path.exists('./plots')==False:\n",
    "        os.mkdir('./plots')\n",
    "    s_pred, s_true = build_visualizable_structures(model, dataloader['train'], config, device)\n",
    "    s_pred.to_pdb(idx,path='./plots/{}_pred.pdb'.format(idx))\n",
    "    s_true.to_pdb(idx,path='./plots/{}_true.pdb'.format(idx))\n",
    "    plot_protein('./plots/{}_pred.pdb'.format(idx), './plots/{}_true.pdb'.format(idx))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    config = get_parameters()\n",
    "    print(\"Model Configuration: \")\n",
    "    print(config)\n",
    "    # Load the data in the appropriate format for training.\n",
    "    dataloader = scn.load(\n",
    "                with_pytorch=\"dataloaders\",\n",
    "                batch_size=config.batch,\n",
    "                dynamic_batching=False)\n",
    "    if config.train:\n",
    "        main(config, dataloader)\n",
    "    else:\n",
    "        plot(config.idx, dataloader, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3e21a5-4271-4078-ba58-c0f482235fb2",
   "metadata": {},
   "source": [
    "#BI4\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "df = pd.read_csv('polymerase_cluster-BI.csv')\n",
    "df.head()\n",
    "\n",
    "df.describe()\n",
    "\n",
    "df.info()\n",
    "\n",
    "df.isna().sum()\n",
    "\n",
    "df.columns\n",
    "\n",
    "X = df[['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
    "       '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22',\n",
    "       '23', '24', '25', '26', '27', '28', '29']]\n",
    "y = df[['G1', 'G2', 'G3', 'G4', 'G5',\n",
    "       'G6', 'G7', 'G8', 'G9', 'G10']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of the model : {accuracy * 100}%\")\n",
    "\n",
    "# Accuracy Score\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.bar(['Accuracy'], [accuracy * 100], color='skyblue')\n",
    "plt.ylim(0, 100)\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.show()\n",
    "\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report :\")\n",
    "print(class_report)\n",
    "\n",
    "# Confusion Matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test.values.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm).plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05214725-7b38-4fce-9e5e-3942edb0478b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
